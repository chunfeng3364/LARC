#! /usr/bin/env python3
# vim:fenc=utf-8
#
# Copyright Â© 2023 Joy Hsu <joycj@stanford.edu>
#
# Distributed under terms of the MIT license.

import contextlib
import collections

import torch
import torch.nn.functional as F
import jactorch
import jactorch.nn as jacnn
from semantics.implementation import QSImplementation
from models.utils.visual_reasoning_utils import VisualConceptInferenceCache
from models.losses import MultitaskLossBase, MultilabelSigmoidCrossEntropyAndAccuracy
from datasets.definition import gdef

__all__ = ['ReferIt3DQSGrounding', 'ReferIt3DQSExecutor', 'RefExpLoss', 'SceneConceptLoss']


class ReferIt3DQSGrounding(VisualConceptInferenceCache):
    def __init__(self, raw_features, embedding_registry, training, input_objects_class, class_to_idx):
        super().__init__()
        self.raw_features = raw_features
        self.embedding_registry = embedding_registry
        self.train(training)
        
        self.input_objects_class = input_objects_class
        cleaned_class_to_idx = {}
        for k in class_to_idx.keys():
            cleaned_class_to_idx[k.replace(' ', '_')] = class_to_idx[k]
        self.class_to_idx = cleaned_class_to_idx        

    def ones_value(self, *shape, log=True):
        offset = 10 if log else 1
        tensor = torch.zeros(shape, dtype=torch.float32, device=self.get_device()) + offset
        return tensor

    def get_device(self):
        return self.raw_features['attribute'].device

    def get_nr_objects(self):
        return self.raw_features['attribute'].size(0)
    
    @VisualConceptInferenceCache.cached_result('filter')
    def infer_filter(self, concept_cat, concept):
        if concept_cat == 'attribute':
            idx = self.class_to_idx[concept]
            mask = self.raw_features[concept_cat][:, idx]
        elif concept_cat == 'multi_relation':
            if concept in self.compose_concept:
                left_mask = self.get_embedding_mod("relation").similarity(self.raw_features["relation"], "left")
                right_mask = self.get_embedding_mod("relation").similarity(self.raw_features["relation"], "right")
                object_len = right_mask.size(0)
                mask = torch.zeros(object_len, object_len, object_len).to(self.raw_features["relation"].device)
                left_mask_ij = left_mask.unsqueeze(2).repeat(1, 1, object_len)
                right_mask_ik = right_mask.unsqueeze(1).repeat(1, object_len, 1)
                left_mask_ik = left_mask.unsqueeze(1).repeat(1, object_len, 1)
                right_mask_ij = right_mask.unsqueeze(2).repeat(1, 1, object_len)
                mask = torch.max(left_mask_ij+right_mask_ik, left_mask_ik+right_mask_ij)
            else:
                feat = self.raw_features['multi_relation']
                object_len = feat.size(0)
                feat = torch.cat([
                    jactorch.add_dim(feat, 0, object_len),
                    jactorch.add_dim(feat, 1, object_len),
                    jactorch.add_dim(feat, 2, object_len)
                ], dim=3)
                mask = self.get_embedding_mod(concept_cat).similarity(feat, concept)
        else: # relation
            mask = self.get_embedding_mod(concept_cat).similarity(self.raw_features[concept_cat], concept)
            
        return mask

    @VisualConceptInferenceCache.cached_result('same')
    def infer_same(self, concept_cat, attribute):
        mask = self.get_embedding_mod(concept_cat).cross_similarity(self.raw_features[concept_cat], attribute)
        return mask

    def get_embedding_mod(self, concept_cat):
        embedding_mod_name = concept_cat + '_embedding'
        return getattr(self.embedding_registry, embedding_mod_name)


class ReferIt3DQSExecutor(QSImplementation):
    def __init__(self, type_system):
        super().__init__(type_system)
        self.grounding = None

    @contextlib.contextmanager
    def with_grounding(self, grounding):
        self.grounding = grounding
        yield
        self.grounding = None

    def scene(self) -> 'object_set':
        return self.grounding.ones_value(self.grounding.get_nr_objects())

    def filter(self, obj: 'object_set', p: 'object_property') -> 'object_set':
        mask = self.grounding.infer_filter('attribute', p.value)
        return torch.min(obj.value, mask)
    
    def relate(self, obj_left: 'object_set', obj_right: 'object_set', r: 'object_relation') -> 'object_set':
        mask = F.softmax(obj_right.value, dim=-1) @ _do_apply_self_mask(self.grounding.infer_filter('relation', r.value))
        return torch.min(obj_left.value, mask)
    
    def relate_multi(self, obj_target: 'object_set', obj_side1: 'object_set', obj_side2: 'object_set', r: 'object_relation') -> 'object_set':
        between_mask = self.grounding.infer_filter('multi_relation', r.value)
        mask = torch.einsum('ijk,j,k->i', between_mask, F.softmax(obj_side1.value), F.softmax(obj_side2.value))
        return torch.min(obj_target.value, mask)

    def relate_anchor(self, obj_target: 'object_set', obj_side: 'object_set', obj_anchor: 'object_set', r: 'object_relation') -> 'object_set':
        anchor_mask = self.grounding.infer_filter('multi_relation', r.value) 
        mask = torch.einsum('ijk,j,k->i', anchor_mask, F.softmax(obj_side.value), F.softmax(obj_anchor.value))
        return torch.min(obj_target.value, mask)


def _get_self_mask(m):
    self_mask = torch.eye(m.size(-1), dtype=m.dtype, device=m.device)
    return self_mask


def _do_apply_self_mask(m):
    self_mask = _get_self_mask(m)
    return m * (1 - self_mask) + (-10) * self_mask


def _logit_softmax(tensor, dim=-1):
    return F.log_softmax(tensor, dim=dim) - torch.log1p(-F.softmax(tensor, dim=dim).clamp(max=1-1e8))


class RefExpLoss(jacnn.TorchApplyRecorderMixin, MultitaskLossBase):
    def __init__(self, add_supervision=True, softmax=False, one_hot=False):
        super().__init__()
        self.add_supervision = add_supervision
        self.softmax = softmax
        self.one_hot = one_hot
        self._multilabel_sigmoid_xent_loss_acc = MultilabelSigmoidCrossEntropyAndAccuracy(
            compute_loss=self.add_supervision,
            softmax=softmax,
            one_hot=self.one_hot
        )

    def forward(self, input, target, input_objects_length):
        monitors = dict()
        outputs = dict()

        batch_size = len(input)
        loss, acc, acc_instance = 0, 0, 0
        for i in range(batch_size):
            this_input = input[i]
            this_target = target[i]

            l = self._batched_xent_loss(this_input, this_target)
            a = float(torch.argmax(this_input) == this_target)     
            ai = (this_input[this_target] > 0).float()

            loss += l
            acc += a
            acc_instance += ai

        if self.training and self.add_supervision:
            monitors['loss/refexp'] = loss / batch_size
            
        if self.training:
            monitors['acc/refexp'] = acc / batch_size
            monitors['acc/refexp/instance'] = acc_instance / batch_size
        else:
            monitors['validation/acc/refexp'] = acc / batch_size
            monitors['validation/acc/refexp/instance'] = acc_instance / batch_size
        
        return monitors, outputs


class SceneConceptLoss(MultitaskLossBase):
    def __init__(self, add_supervision=False):
        super().__init__()
        self.add_supervision = add_supervision

    def forward(self, feed_dict, f_sng, attribute_embedding, referred_objs, reference_objs):
        outputs, monitors = dict(), dict()

        objects = [f['attribute'] for f in f_sng]
        all_f = torch.stack(objects)
        object_labels = feed_dict['input_objects_class']
        idx_to_class = {v: k for k, v in feed_dict['class_to_idx'][0].items()}
        
        if not self.training: # logs
            objs_pred_as_referred_objs, objs_pred_as_reference_objs = [], []
            for b in range(all_f.size(0)):
                this_object_pred = all_f[b, :, :]
                this_referred_obj = feed_dict['class_to_idx'][0][referred_objs[b].replace('_', ' ')]
                this_reference_obj = feed_dict['class_to_idx'][0][reference_objs[b].replace('_', ' ')]

                objs_pred_as_referred_obj_orig = this_object_pred[:, this_referred_obj]
                objs_pred_as_reference_obj_orig = this_object_pred[:, this_reference_obj]
                objs_pred_as_referred_obj_orig = F.softmax(objs_pred_as_referred_obj_orig)
                objs_pred_as_reference_obj_orig = F.softmax(objs_pred_as_reference_obj_orig)
                objs_pred_as_referred_obj = list(range(objs_pred_as_referred_obj_orig.size(0)))
                objs_pred_as_reference_obj = list(range(objs_pred_as_reference_obj_orig.size(0)))
                this_obj_labels = object_labels[b]
                this_obj_labels = [idx_to_class[int(o_l.cpu().numpy())] for o_l in this_obj_labels]

                objs_pred_as_referred_obj_vals = [str(round(float(objs_pred_as_referred_obj_orig[o_i].cpu().numpy()), 4)) for o_i in objs_pred_as_referred_obj]
                objs_pred_as_referred_obj = [this_obj_labels[o_i] for o_i in objs_pred_as_referred_obj]
                fin_objs_pred_as_referred_obj = referred_objs[b] + ': '
                referred_zipped = zip(objs_pred_as_referred_obj, objs_pred_as_referred_obj_vals)
                for this_o, this_v in sorted(referred_zipped, reverse=True, key = lambda t: t[1]):
                    fin_objs_pred_as_referred_obj += this_o +  ' (' + this_v + '), '  
                objs_pred_as_referred_objs.append(fin_objs_pred_as_referred_obj)

                objs_pred_as_reference_obj_vals = [str(round(float(objs_pred_as_reference_obj_orig[o_i].cpu().numpy()), 4)) for o_i in objs_pred_as_reference_obj]
                objs_pred_as_reference_obj = [this_obj_labels[o_i] for o_i in objs_pred_as_reference_obj]
                fin_objs_pred_as_reference_obj = reference_objs[b] + ': '
                reference_zipped = zip(objs_pred_as_reference_obj, objs_pred_as_reference_obj_vals)
                for this_o, this_v in sorted(reference_zipped, reverse=True, key = lambda t: t[1]):
                    fin_objs_pred_as_reference_obj += this_o +  ' (' + this_v + '), '  
                objs_pred_as_reference_objs.append(fin_objs_pred_as_reference_obj)

            outputs['objs_pred_as_referred_obj'] = objs_pred_as_referred_objs 
            outputs['objs_pred_as_reference_obj'] = objs_pred_as_reference_objs 
        
        all_scores = []
        for concept in gdef.attribute_concepts:
            concept = concept.replace('_', ' ')
            if concept in feed_dict['class_to_idx'][0]:
                this_idx = feed_dict['class_to_idx'][0][concept]
                if this_idx == 607: # pad
                    all_scores.append(torch.zeros(all_f.size(0), all_f.size(1)).cuda())
                else:
                    this_score = all_f[:, :, this_idx]
                    all_scores.append(this_score)
            else:
                all_scores.append(torch.zeros(all_f.size(0), all_f.size(1)).cuda())
        all_scores = torch.stack(all_scores, dim=-1)
        
        accs, losses = [], []
        concepts_to_accs, concepts_to_pred_concepts = [], []
        for b in range(object_labels.size(0)):
            curr_concepts_to_accs = collections.defaultdict(list)
            curr_concepts_to_pred_concepts = collections.defaultdict(list)
            for i in range(object_labels.size(1)):
                curr_label = int(object_labels[b, i].cpu().numpy())
                curr_class = idx_to_class[curr_label]
                curr_class = curr_class.replace(' ', '_')
                if curr_class in gdef.attribute_concepts:
                    curr_class_index = gdef.attribute_concepts.index(curr_class)
                
                pred_scores_for_object = all_scores[b, i, :] 
                if curr_class != 'pad':
                    this_max_class = int(torch.argmax(pred_scores_for_object).cpu().numpy())
                    this_pred_class = gdef.attribute_concepts[this_max_class]
                    curr_concepts_to_pred_concepts[curr_class].append(this_pred_class)
                    this_acc = float(this_max_class == curr_class_index)               
                    accs.append(this_acc)
                    curr_concepts_to_accs[curr_class].append(this_acc)

                this_loss = torch.nn.CrossEntropyLoss(ignore_index=feed_dict['class_to_idx'][0]['pad'])(pred_scores_for_object, torch.tensor(curr_class_index).cuda())
                losses.append(this_loss)
            
            concepts_to_accs.append(curr_concepts_to_accs)
            concepts_to_pred_concepts.append(curr_concepts_to_pred_concepts)
        
        avg_acc = sum(accs) / len(accs)
        avg_loss = sum(losses) / len(losses)
        
        acc_for_refs, acc_for_references = [], []
        for i, (ref, reference) in enumerate(zip(referred_objs, reference_objs)):
            if ref in concepts_to_accs[i]:
                curr_acc_for_ref = concepts_to_accs[i][ref]
                curr_acc_for_ref = sum(curr_acc_for_ref) / len(curr_acc_for_ref)
                acc_for_refs.append(round(curr_acc_for_ref, 3))
            else:
                acc_for_refs.append(0.0)
            
            if reference in concepts_to_accs[i]:
                curr_acc_for_references = concepts_to_accs[i][reference]
                if len(curr_acc_for_references) == 0:
                    acc_for_references.append(None)
                else:
                    curr_acc_for_references = sum(curr_acc_for_references) / len(curr_acc_for_references)
                    acc_for_references.append(round(curr_acc_for_references, 3))
            else:
                acc_for_references.append(0.0)
        
        outputs['referred_objs'] = referred_objs
        outputs['anchor_objs'] = reference_objs
        outputs['acc_for_refs'] = acc_for_refs
        outputs['acc_for_ancs'] = acc_for_references
        outputs['concepts_to_accs'] = concepts_to_accs
        outputs['concepts_to_pred_concepts'] = concepts_to_pred_concepts
        
        if self.training and self.add_supervision:
            monitors['loss/object_cls'] = avg_loss
            
        if self.training:
            monitors['acc/object_cls'] = avg_acc
            monitors['train/acc/object_cls'] = avg_acc
        else:
            monitors['validation/acc/object_cls'] = avg_acc

        return monitors, outputs
    

class Regularization(jacnn.TorchApplyRecorderMixin, MultitaskLossBase):
    def __init__(self):
        super().__init__()
        self.sparsity_loss = torch.nn.L1Loss(size_average=None, reduce=None, reduction="mean")
    
    def forward(self, excl_rel, sym_rel, spa_rel):
        monitors = dict()
        outputs = dict()

        num_excl, num_sym, num_spa = len(excl_rel), len(sym_rel), len(spa_rel)
        excl_loss, sym_loss, spa_loss = 0.0, 0.0, 0.0
        
        for i in range(num_excl):
            excl_concept = F.relu(excl_rel[i])
            excl_loss += torch.norm(torch.mul(excl_concept, excl_concept.transpose(0, 1)).masked_fill_(torch.eye(excl_concept.size(0)).bool().to(excl_concept.device), 0.0), p=2)
        for i in range(num_sym):
            sym_concept = sym_rel[i]
            sym_loss += torch.norm((sym_concept - sym_concept.transpose(0, 1)), p=2)
        for i in range(num_spa):
            spa_concept = spa_rel[i]
            gt_matrix = torch.zeros_like(spa_concept)
            num_object = spa_concept.size(0)
            if len(spa_concept.size()) == 2:
                eye_mask = torch.eye(num_object, num_object).bool()
                gt_matrix[eye_mask] = spa_concept[eye_mask]  
            elif len(spa_concept.size()) == 3:
                for idx_i in range(num_object):
                    gt_matrix[idx_i, idx_i, idx_i] = spa_concept[idx_i, idx_i, idx_i]
            spa_loss += self.sparsity_loss(spa_concept, gt_matrix)
        
        if self.training:
            monitors['loss/excl'] = excl_loss / num_excl if num_excl > 0 else 0.0
            monitors['loss/sym'] = sym_loss / num_sym if num_sym > 0 else 0.0
            monitors['loss/spa'] = spa_loss / num_spa if num_spa > 0 else 0.0
            monitors['loss/regular'] = monitors['loss/excl'] + monitors['loss/sym'] + 0.1*monitors['loss/spa']
        else:
            monitors['validation/loss/excl'] = excl_loss / num_excl if num_excl > 0 else 0.0
            monitors['validation/loss/sym'] = sym_loss / num_sym if num_sym > 0 else 0.0
            monitors['validation/loss/spa'] = spa_loss / num_spa if num_spa > 0 else 0.0
            monitors['validation/loss/regular'] = monitors['validation/loss/excl'] + monitors['validation/loss/sym'] + 0.1*monitors['validation/loss/spa']
        
        return monitors, outputs